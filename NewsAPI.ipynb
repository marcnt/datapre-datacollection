{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Articles from an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous notebook demonstrated how to fetch posts from Reddit.\n",
    "\n",
    "This notebook, however, will demonstrate how to fetch articles from a news site through their API. Specifically, we will be getting articles from [The Guardian](https://www.theguardian.com/), which offers an [API](https://open-platform.theguardian.com/) from which we can get articles. Registration is easy and is free for developer use, which is enough for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "\n",
    "To make web requests to the API, we need to import `requests` and to process JSON, we import `json`.\n",
    "\n",
    "Also, we will define a utility function `request_api()` to wrap the request process. The `endpoint` parameter denotes which endpoint will be called. For example, the `search` endpoint returns content. The `apiKey` parameter is our API key, which is required anyway. The `params` keyword arguments are basically the parameters that the endpoints can accept. This modifies the URL in a manner similar to when making GET requests (e.g. `http://example.com?key=value&key2=value2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def request_api(endpoint, apiKey, **params):\n",
    "    API_SITE = \"https://content.guardianapis.com/\"\n",
    "    queries = 'api-key=' + apiKey\n",
    "    \n",
    "    for key, value in params.items():\n",
    "        queries = key.replace('_', '-') + '=' + str(value) + '&' + queries\n",
    "    \n",
    "    url = API_SITE + endpoint + '?' + queries\n",
    "    return requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the API key\n",
    "\n",
    "To be able to access the API, we need to read the key. This key only allows a limited number of requests per day, so it is important to keep the requests down as much as possible. Also, it must not be pushed into the repository for obvious reasons.\n",
    "\n",
    "One way to do this is to store the key in a file named `key.txt`, then putting the file name in a file named `.gitignore`, which will prevent git from committing the key file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = ''\n",
    "with open('key.txt', 'r') as file:\n",
    "    API_KEY = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with the API\n",
    "\n",
    "Of interest is the `/search` endpoint, which we will use to fetch news articles. It supports parameters like how many results per 'page', the number of pages and the dates. More information can be found in the [documentation](https://open-platform.theguardian.com/documentation/).\n",
    "\n",
    "Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = request_api('search', API_KEY)\n",
    "test = json.loads(content.text)\n",
    "len(test['response']['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `/search` returns ten results. If we want more results per page, we set the page size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = request_api('search', API_KEY, page_size=20)\n",
    "test = json.loads(content.text)\n",
    "len(test['response']['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents state that the maximum accepted value for `page-size` is 50. We will use that value.\n",
    "\n",
    "To read more pages, we simply set `page` to whatever page we are on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the articles\n",
    "\n",
    "What we need is to return all news articles from 11 March to 12 March 2021. Fortunately, the `/search` endpoint allows the parameters `from-date` and `to-date` which specifies the date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = request_api('search', API_KEY, page_size=50, from_date='2021-03-11', to_date='2021-03-12')\n",
    "test = json.loads(content.text)\n",
    "test['response']['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the article's author(s). These are typically the article's contributors or whoever contributed to the story or article. By default, the requests do not return contributor information; they are stored in tags that need to be explicitly requested. To do this, we put `show-tags=contributor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = request_api('search', API_KEY, page_size=50, from_date='2021-03-11', to_date='2021-03-12', show_tags='contributor')\n",
    "test = json.loads(content.text)\n",
    "test['response']['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we need the full text of the article. The documentation says one can extract the full text using `show-fields=body`, but it is in HTML and we do not need tags floating in our text.\n",
    "\n",
    "We can extract the full, non-HTML text by specifying `show-blocks=body`, and reading `bodyTextSummary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = request_api('search', API_KEY, page_size=50, from_date='2021-03-11', to_date='2021-03-12', show_tags='contributor', show_blocks='body')\n",
    "test = json.loads(content.text)\n",
    "test['response']['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, some entries do not refer to news articles. For example, there are `liveblog` entries, which means something is being tracked, for example, in November 2020, the US elections which was frequently updated with new entries as fresh news came.\n",
    "\n",
    "We want to filter the `type` so that we are only reading `article` entries. So, while we are reading each entry, we check `type` if it is `article`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles = list(filter(lambda x: x['type'] == 'article', test['response']['results']))\n",
    "test_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to read entries into objects so we can save them as a JSON file. To start, we process the first entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title = test_articles[0]['webTitle']\n",
    "test_date = test_articles[0]['webPublicationDate']\n",
    "test_authors = []\n",
    "for tag in test_articles[0]['tags']:\n",
    "    if tag['type'] != 'contributor':\n",
    "        continue\n",
    "    test_authors.append(tag['webTitle'])\n",
    "    \n",
    "# For article entries, the number of blocks is only one, so it should be easy to extract the text.\n",
    "test_text = test_articles[0]['blocks']['body'][0]['bodyTextSummary']\n",
    "\n",
    "print(test_title)\n",
    "print(test_date)\n",
    "print(test_authors)\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be something wrong with the text, probably caused by the encoding. Fortunately, we don't need fancy apostrophes and quotation marks. We will have to clean it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Could be cleaner\n",
    "    return text.replace(\"â€™\", \"'\").replace(\"â€œ\", \"\\\"\").replace(\"â€�\", \"\\\"\").replace(\"â€¢\", \"*\").replace(\"Ã©\", \"e\").replace(\"Ã¼\", \"u\").replace(\"â€“\", \"-\")\n",
    "\n",
    "print(clean_text(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same must be done on any field that uses special formatting, like the title of the article.\n",
    "\n",
    "Now that we have done cleaning chores, let's compile everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article_objs = []\n",
    "for article in test_articles:\n",
    "    # Permit only articles, not liveblogs or other\n",
    "    if article['type'] != 'article':\n",
    "        continue\n",
    "    \n",
    "    title = clean_text(article['webTitle'])\n",
    "    date = article['webPublicationDate']\n",
    "    authors = []\n",
    "    for tag in article['tags']:\n",
    "        if tag['type'] != 'contributor':\n",
    "            continue\n",
    "        authors.append(clean_text(tag['webTitle']))\n",
    "\n",
    "    # For article entries, the number of blocks is only one, so it should be easy to extract the text.\n",
    "    text = clean_text(article['blocks']['body'][0]['bodyTextSummary'])\n",
    "    \n",
    "    test_article_objs.append({\n",
    "        'title': title,\n",
    "        'date': date,\n",
    "        'authors': authors,\n",
    "        'text': text\n",
    "    })\n",
    "    \n",
    "test_article_objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "We need to collect as many articles as we could between 11 and 12 March 2021. The requests can only support a maximum number of results per page, so we need to loop as much as we can, and apply an artificial limit so we do not overload the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_limit = 20\n",
    "from_date = '2021-03-11'\n",
    "to_date = '2021-03-12'\n",
    "page_limit = 50\n",
    "all_articles = []\n",
    "\n",
    "for page in range(1, max_limit + 1):\n",
    "    print(\"Fetching page \" + str(page))\n",
    "    try:\n",
    "        response = request_api('search', API_KEY, page=page, page_size=page_limit, from_date=from_date, to_date=to_date, show_tags='contributor', show_blocks='body')\n",
    "        \n",
    "        # Avoid upsetting the API :(\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        \n",
    "        results = json.loads(response.text)['response']['results']\n",
    "        \n",
    "        for entry in results:\n",
    "            # Permit only articles, not liveblogs or other\n",
    "            if entry['type'] != 'article':\n",
    "                continue\n",
    "\n",
    "            title = clean_text(entry['webTitle'])\n",
    "            date = entry['webPublicationDate']\n",
    "            authors = []\n",
    "            for tag in entry['tags']:\n",
    "                if tag['type'] != 'contributor':\n",
    "                    continue\n",
    "                authors.append(clean_text(tag['webTitle']))\n",
    "\n",
    "            # For article entries, the number of blocks is only one, so it should be easy to extract the text.\n",
    "            text = clean_text(entry['blocks']['body'][0]['bodyTextSummary'])\n",
    "\n",
    "            all_articles.append({\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'authors': authors,\n",
    "                'text': text\n",
    "            })\n",
    "            \n",
    "        # We have reached the limit (probably)\n",
    "        if len(results) < page_limit:\n",
    "            break\n",
    "    except:\n",
    "        # If we somehow encounter an error, break instantly\n",
    "        break\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have collected all the articles, we can now format the whole array as JSON and write to the file `articles.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles.json\", \"w\") as f:\n",
    "    json.dump(all_articles, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
